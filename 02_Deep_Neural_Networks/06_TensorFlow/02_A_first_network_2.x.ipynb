{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A First NN with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weights(n_features, n_labels):\n",
    "    \"\"\"\n",
    "    Return TensorFlow weights\n",
    "    :param n_features: Number of features\n",
    "    :param n_labels: Number of labels\n",
    "    :return: TensorFlow weights\n",
    "    \"\"\"\n",
    "    # TODO: Return weights\n",
    "    return tf.Variable(tf.random.truncated_normal((n_features, n_labels)), name=\"W\", trainable=True)\n",
    "\n",
    "\n",
    "def get_biases(n_labels):\n",
    "    \"\"\"\n",
    "    Return TensorFlow bias\n",
    "    :param n_labels: Number of labels\n",
    "    :return: TensorFlow bias\n",
    "    \"\"\"\n",
    "    # TODO: Return biases\n",
    "    return tf.Variable(tf.zeros(n_labels), name=\"b\", trainable=True)\n",
    "\n",
    "\n",
    "def linear(x, w, b):\n",
    "    \"\"\"\n",
    "    Return linear function in TensorFlow\n",
    "    :param x: TensorFlow input\n",
    "    :param w: TensorFlow weights\n",
    "    :param b: TensorFlow biases\n",
    "    :return: TensorFlow linear function\n",
    "    \"\"\"\n",
    "    # TODO: Linear Function (xW + b)\n",
    "    return tf.add(tf.matmul(x, w), b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_features_labels(n_images):\n",
    "    \"\"\"\n",
    "    Gets the first <n> images from the MNIST dataset\n",
    "    \"\"\"\n",
    "    mnist_features = []\n",
    "    mnist_labels = []\n",
    "\n",
    "    (train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data(\n",
    "        \"/files/cedric/datasets/udacity/ai_nanodegree/keras/mnist.npz\")\n",
    "    print(\"Total number of training images: {}\".format(train_images.shape[0]))\n",
    "    print(\"Total number of testing images: {}\".format(train_images.shape[1]))\n",
    "    return train_images[:n_images], train_labels[:n_images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of training images: 60000\n",
      "Total number of testing images: 28\n",
      "Training features: (60000, 784), float32\n",
      "Training labels: (60000, 10), float32\n"
     ]
    }
   ],
   "source": [
    "# Number of features (28*28 image is 784 features)\n",
    "n_features = 784\n",
    "# Number of labels\n",
    "n_labels = 10\n",
    "\n",
    "# Training data\n",
    "train_features, train_labels = mnist_features_labels(60000)\n",
    "train_features = train_features.astype(np.float32).reshape((60000, -1)) / 255. # reshape and normalize\n",
    "print(f\"Training features: {train_features.shape}, {train_features.dtype}\")\n",
    "train_labels = pd.get_dummies(train_labels).values.astype(np.float32)\n",
    "print(f\"Training labels: {train_labels.shape}, {train_labels.dtype}\")\n",
    "\n",
    "# Features and Labels\n",
    "features = tf.convert_to_tensor(train_features, name=\"x\")\n",
    "labels = tf.convert_to_tensor(train_labels, name=\"y\")\n",
    "\n",
    "# Model\n",
    "w = get_weights(n_features, n_labels)\n",
    "b = get_biases(n_labels)\n",
    "\n",
    "# Loss function\n",
    "cce = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "# Optimizer\n",
    "learning_rate = 0.08\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: [[ 0.54152197 -1.1262169   0.29850656 ...  0.22147019  1.069837\n",
      "   0.45944366]\n",
      " [ 0.18941663 -0.04188945 -0.831536   ... -1.2713394  -0.99510354\n",
      "  -0.15013368]\n",
      " [-1.3394647   0.07296699 -0.14267394 ...  0.3440815   1.6246268\n",
      "  -1.1913354 ]\n",
      " ...\n",
      " [ 0.9428878   0.29960948  0.77241534 ...  0.02454096  1.0591465\n",
      "  -0.9935228 ]\n",
      " [ 1.8036964  -0.33023098  0.10627091 ... -0.31757647  0.35502246\n",
      "   0.38793278]\n",
      " [-0.6429783  -0.34071702 -1.8948934  ...  0.73784846 -0.17394464\n",
      "  -0.92118126]]\n",
      "Bias: [-0.00010709  0.0076489  -0.00035305 -0.00205179  0.00264403  0.00017037\n",
      "  0.00038194 -0.00109777 -0.00068567 -0.00654988]\n",
      "Scores: [[  1.0122232   11.665441    -5.674629   ...  14.019657    -8.155622\n",
      "    8.027456  ]\n",
      " [  2.6679661    1.4408989   -2.8300245  ...   3.3340604   -9.67888\n",
      "    4.3264217 ]\n",
      " [ 15.554178     2.21923     -9.6022835  ...  -4.3050623    2.2486703\n",
      "    6.012452  ]\n",
      " ...\n",
      " [ -2.747803    26.859623     6.7662134  ...   5.731075    -0.44590807\n",
      "    1.8070089 ]\n",
      " [ 11.067545     2.5732095    3.4741416  ...   5.9622593  -10.515982\n",
      "    5.2447505 ]\n",
      " [ -0.13971388  15.816455    11.158531   ...  14.858617    -6.4505606\n",
      "    4.8963413 ]]\n",
      "Softmax: [[2.03736818e-06 8.62388685e-02 2.54101029e-09 ... 9.08084631e-01\n",
      "  2.12581550e-10 2.26854067e-03]\n",
      " [1.10016108e-01 3.22513357e-02 4.50514926e-04 ... 2.14159489e-01\n",
      "  4.77847379e-07 5.77715993e-01]\n",
      " [9.99776900e-01 1.61662217e-06 1.18738231e-11 ... 2.37216069e-09\n",
      "  1.66492453e-06 7.17768053e-05]\n",
      " ...\n",
      " [1.38566513e-13 9.99999762e-01 1.87733851e-09 ... 6.66788680e-10\n",
      "  1.38470940e-12 1.31761277e-11]\n",
      " [9.90304828e-01 2.02640382e-04 4.98879584e-04 ... 6.00580964e-03\n",
      "  4.18950180e-10 2.93063279e-03]\n",
      " [8.41772021e-08 7.15930283e-01 6.79141330e-03 ... 2.74717897e-01\n",
      "  1.52907506e-10 1.29516675e-05]]\n",
      "Loss: 14.281830787658691\n",
      "Gradients [w]: [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Gradients [b]: [ 0.00133859 -0.09561126  0.00441315  0.02564739 -0.03305038 -0.00212963\n",
      " -0.00477422  0.01372209  0.00857082  0.08187345]\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def session(features, labels, w, b, cce, optimizer):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Records the gradient during the forward pass\n",
    "        # Linear Function xW + b\n",
    "        logits = linear(features, w, b)   \n",
    "        # Softmax\n",
    "        prediction = tf.nn.softmax(logits)\n",
    "        # Loss\n",
    "        loss = cce(prediction, labels)\n",
    "    # Get the gradients for weights and bias\n",
    "    gradients = tape.gradient(loss, [w, b])\n",
    "    \n",
    "    # Apply optimizer (one step)\n",
    "    optimizer.apply_gradients(zip(gradients, [w, b]))\n",
    "    \n",
    "    # End\n",
    "    return [w, b, logits, prediction, loss, gradients]\n",
    "\n",
    "outputs = session(features, labels, w, b, cce, optimizer)\n",
    "\n",
    "print('Weights: {}'.format(outputs[0]))\n",
    "print('Bias: {}'.format(outputs[1]))\n",
    "print('Scores: {}'.format(outputs[2]))\n",
    "print('Softmax: {}'.format(outputs[3]))\n",
    "print('Loss: {}'.format(outputs[4]))\n",
    "print('Gradients [w]: {}'.format(outputs[5][0]))\n",
    "print('Gradients [b]: {}'.format(outputs[5][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
