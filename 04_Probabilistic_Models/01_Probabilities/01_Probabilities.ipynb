{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilities\n",
    "\n",
    "Complementary probabilities:\n",
    "\n",
    "$\n",
    "P(A) = p \\Rightarrow p(\\lnot A) = 1 - p\n",
    "$\n",
    "\n",
    "Independence:\n",
    "\n",
    "$\n",
    "X \\bot Y:  P(X \\cap Y) = P(X) P(Y) \\\\\n",
    "X \\bot Y:  P(X \\cup Y) = P(X) + P(Y) \\\\\n",
    "$\n",
    "\n",
    "$P(X)$ is the marginal probability, $P(X,Y)$ is the joint probability. $X \\cap Y$ X and Y events both happen, $X \\cup Y$ either X or Y event happens.\n",
    "\n",
    "Dependence:\n",
    "\n",
    "$\n",
    "P(Y) = \\sum_{i} P(Y|X=i) P(X=i) \\\\\n",
    "P(\\lnot Y|X) = 1 - P(Y|X)\n",
    "$\n",
    "\n",
    "$\n",
    "P(Y, X) = P(Y|X) \\cdot P(X)\n",
    "$\n",
    "\n",
    "Note that the later can be written this way:\n",
    "\n",
    "$\n",
    "P(A,B,C|D,E) = \\large \\frac{P(A,B,C,D,E)}{P(D,E)}\n",
    "$\n",
    "\n",
    "In a Bayes network, A, B and C are the query variables. D and E are the evidence. The conditional probabilities of the queries given the evidences is the ratio between the joint probability of all variables over the joint probabilities of the evidences.\n",
    "\n",
    "If X and Y are independent:\n",
    "\n",
    "$\n",
    "P(Y|X) = P(Y)\n",
    "$\n",
    "\n",
    "and so you find the independent joint probability relationship:\n",
    "\n",
    "$\n",
    "P(Y, X) = P(Y) \\cdot P(X)\n",
    "$\n",
    "\n",
    "Bayes rule:\n",
    "\n",
    "$\n",
    "P(B|A) = \\large\\frac{P(A|B) P(B)}{P(A)}\n",
    "$\n",
    "\n",
    "$P(B|A)$ is the *posterior* probability. $P(A|B)$ is the *likelihood*. $P(B)$ is the *prior*. $P(A)$ is the *marginal likelihood*.\n",
    "\n",
    "If now we have 3 events, Bayes rule can be written in two different ways:\n",
    "\n",
    "$\n",
    "P(B|A,C) = \\large\\frac{P(A,C|B) \\cdot P(B)}{P(A,C)}\n",
    "$\n",
    "\n",
    "or, if C conditions all events:\n",
    "\n",
    "$\n",
    "P(B|A,C) = \\large\\frac{P(A|B,C) \\cdot P(B|C)}{P(A|C)}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of Bayes rule\n",
    "\n",
    "## Problem definition\n",
    "\n",
    "**C**: is having the cancer.<br>\n",
    "**+**: is being tested positive to cancer.<br>\n",
    "**-**: is being tested negative to cancer ($\\lnot +$).<br>\n",
    "\n",
    "In the population, only 1% has cancer:\n",
    "\n",
    "$\n",
    "P(C) = 0.01 \\\\\n",
    "P(\\lnot C) = 0.99\n",
    "$\n",
    "\n",
    "The test of cancer is as follows:\n",
    "\n",
    "$\n",
    "P(+|C) = 0.9 \\\\\n",
    "P(-|C) = 0.1\n",
    "$\n",
    "\n",
    "$P(+|C)$ is the **true positive rate** or **sensitivity**\n",
    "\n",
    "and\n",
    "\n",
    "$\n",
    "P(-|\\lnot C) = 0.8 \\\\\n",
    "P(+|\\lnot C) = 0.2\n",
    "$\n",
    "\n",
    "$P(-|\\lnot C)$ is the **true negative rate** or **specificity**\n",
    "\n",
    "## Probability of having cancer if tested positive\n",
    "\n",
    "The Bayes rule enables to answer the following question: how is it likelly to have cancer when you are tested positive ? This is $P(C|+)$.\n",
    "\n",
    "$\n",
    "P(C|+) = \\large\\frac{P(+|C) P(C)}{P(+)}\n",
    "$\n",
    "\n",
    "$P(C|+)$ is the posterior probability. $P(C)$ is the *prior* probability. $P(+|C)$ is the likelihood. $P(+)$ is the marginal likelihood.\n",
    "\n",
    "Knowing with the total probability that:\n",
    "\n",
    "$\n",
    "P(+) = P(+|C) P(C) + P(+|\\lnot C) P(\\lnot C)\n",
    "$\n",
    "\n",
    "then\n",
    "\n",
    "$\n",
    "P(C|+) = \\large\\frac{P(+|C) P(C)}{P(+)} = \\large\\frac{P(+|C) P(C)}{P(+|C) P(C) + P(+|\\lnot C) P(\\lnot C)}\n",
    "$\n",
    "\n",
    "Subsituted with actual values:\n",
    "\n",
    "$\n",
    "P(C|+) = \\large\\frac{P(+|C) P(C)}{P(+|C) P(C) + P(+|\\lnot C) P(\\lnot C)} = \\large\\frac{0.9 \\cdot 0.01}{0.9 \\cdot 0.01 + 0.2 \\cdot 0.99} \\approx 0.043\n",
    "$\n",
    "\n",
    "This value is pretty low. this is because the test has 20% chances of giving a false positive. If we reduce the false positive down to 0.001 (one for a thousand):\n",
    "\n",
    "$\n",
    "P(C|+) = \\large\\frac{P(+|C) P(C)}{P(+|C) P(C) + P(+|\\lnot C) P(\\lnot C)} = \\large\\frac{0.9 \\cdot 0.01}{0.9 \\cdot 0.01 + 0.0001 \\cdot 0.99} \\approx 0.99\n",
    "$\n",
    "\n",
    "## Probability of having cancer if tested positive twice\n",
    "\n",
    "Now let's assume that we are tested twice (with the same test). What is the probability of having cancer if both tests are positive ? This probabality is $P(C|+,+)$.\n",
    "\n",
    "The Bayes rule leads to:\n",
    "\n",
    "$\n",
    "P(C|+,+) = \\large \\frac{P(+,+|C) \\cdot P(C)}{P(+,+)}\n",
    "$\n",
    "\n",
    "$P(+,+)$ might be tricky to calculate, so we introduce the following trick:\n",
    "\n",
    "$\n",
    "P(C|+,+) = \\large \\frac{P(+,+|C) \\cdot P(C)}{P(+,+)} \\\\\n",
    "P(\\lnot C|+,+) = \\large \\frac{P(+,+| \\lnot C) \\cdot P(\\lnot C)}{P(+,+)}\n",
    "$\n",
    "\n",
    "We know that this two propabilities $P(C|+,+)$ and $P(\\lnot C|+,+)$ should sum to 1, so we only calculate the following terms:\n",
    "\n",
    "$\n",
    "P'(C|+,+) = P(+,+|C) \\cdot P(C) = P(+|C) \\cdot P(+|C) \\cdot P(C) = 0.9 \\cdot 0.9 \\cdot 0.01 \\approx 0.0081 \\\\\n",
    "P'(\\lnot C|+,+) = P(+,+|\\lnot C) \\cdot P(\\lnot C) = P(+|\\lnot C) \\cdot P(+|\\lnot C) \\cdot P(C) \\approx 0.0396\n",
    "$\n",
    "\n",
    "and so:\n",
    "\n",
    "$\n",
    "P(C|+,+) = \\large \\frac{P(C|+,+)}{P(C|+,+) + P(\\lnot C|+,+)} \\approx 0.1698 \\\\\n",
    "P(\\lnot C|+,+) = \\large \\frac{P(\\lnot C|+,+)}{P(C|+,+) + P(\\lnot C|+,+)} \\approx 0.8302\n",
    "$\n",
    "\n",
    "So the probability of having cancer knowing that two tests are positive is almost 17%.\n",
    "\n",
    "## Probability of having cancer if tested positive and negative (once each)\n",
    "\n",
    "We now assume that one test is positive and the other is negative. What is the probability of having cancer, noted $P(C|+,-)$ ? By analogy with the previous case we can write:\n",
    "\n",
    "$\n",
    "P'(C|+,-) = P(+,-|C) \\cdot P(C) = P(+|C) \\cdot P(-|C) \\cdot P(C) = 0.9 \\cdot 0.1 \\cdot 0.01 \\approx 0.0009 \\\\\n",
    "P'(\\lnot C|+,-) = P(+,-|\\lnot C) \\cdot P(\\lnot C) = P(+|\\lnot C) \\cdot P(-|\\lnot C) \\cdot P(C) \\approx 0.1584\n",
    "$\n",
    "\n",
    "and so:\n",
    "\n",
    "$\n",
    "P(C|+,-) = \\large \\frac{P(C|+,-)}{P(C|+,-) + P(\\lnot C|+,-)} \\approx 0.0056 \\\\\n",
    "P(\\lnot C|+,+) = \\large \\frac{P(\\lnot C|+,-)}{P(C|+,-) + P(\\lnot C|+,-)} \\approx 0.9944\n",
    "$\n",
    "\n",
    "So the probability of having cancer knowing that one test is positive while the other is negative is 0.56%.\n",
    "\n",
    "## Conditional independance\n",
    "\n",
    "In the two previous examples we have assumed:\n",
    "\n",
    "$\n",
    "P(+,-|C) \\cdot = P(+|C) \\cdot P(-|C)\n",
    "P(+,+|C) \\cdot = P(+|C) \\cdot P(+|C)\n",
    "$\n",
    "\n",
    "This is because, given C, + and - are independent. We say that + and - are conditionnaly independent:\n",
    "\n",
    "$\n",
    "+ \\bot - | C\n",
    "$\n",
    "\n",
    "But this does not mean that they are independent whatever the circumstances because the cause (having cancer or not affect both). **Conditional independance does not imply absolute independence**.\n",
    "\n",
    "## Probability of having a second test positive knowing the first one was positive\n",
    "\n",
    "This probability will be written $P(+_2|+_1)$:\n",
    "\n",
    "$\n",
    "P(+_2|+_1) = P(+_2|+_1, C) \\cdot P(C|+_1) + P(+_2|+_1, \\lnot C) \\cdot P(\\lnot C|+_1)\n",
    "$\n",
    "\n",
    "We need to take into consideration that $+_1$ and $+_2$ are conditionnaly independent so we can write:\n",
    "\n",
    "$\n",
    "P(+_2|+_1) = P(+_2|C) \\cdot P(C|+_1) + P(+_2|\\lnot C) \\cdot P(\\lnot C|+_1)\n",
    "$\n",
    "\n",
    "Said differently, the first test might have been positive while we have cancer or being positive while we haven't cancer. This is a rewrite of the total probability theorem but given a condition $+_1$. \n",
    "\n",
    "\n",
    "With a single test the probability would have been written:\n",
    "\n",
    "$\n",
    "P(+) = P(+|C) \\cdot P(C) + P(+|\\lnot C) \\cdot P(\\lnot)\n",
    "$\n",
    "\n",
    "Subsituting with actual values:\n",
    "\n",
    "$\n",
    "P(+_2|+_1) = P(+_2|C) \\cdot P(C|+_1) + P(+_2|\\lnot C) \\cdot P(\\lnot C|+_1) = 0.9 \\cdot 0.043 + 0.2 \\cdot 0.957 \\approx 0.2301\n",
    "$\n",
    "\n",
    "The second test has 23% of chances to be positive knowing that the first test was positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cause and effect with Bayes rule\n",
    "\n",
    "In the cancer example below, the cause is having or not the cancer and the effect is the test being positive or negative. The Bayes rule enables then to analyse the cause from the effect:\n",
    "\n",
    "What is the probability of having cancer (cause) given the test (effect):\n",
    "\n",
    "$\n",
    "P(C|+) = \\large\\frac{P(+|C) P(C)}{P(+)}\n",
    "$\n",
    "\n",
    "And this probability is dependent on the probability of the test being positive (effect) knowing the individual has cancer (cause). \n",
    "\n",
    "The Bayes rule then enables to analyse cause from the effect and inversly. It enables to pass from what we know (effect) to what we infer (cause).\n",
    "\n",
    "The Bayes theorem can be read as follows:\n",
    "\n",
    "$\n",
    "P(cause|effect) = \\large\\frac{P(effect|cause) P(cause)}{P(effect)}\n",
    "$\n",
    "\n",
    "$P(cause|effect)$ is the *posterior* probability (what we have inferred). $P(effect|cause)$ is the *likelihood*. $P(cause)$ is the *prior* (what we knew before). $P(effect)$ is the *marginal likelihood*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deferred normalizer\n",
    "\n",
    "The Bayes rule:\n",
    "\n",
    "$\n",
    "P(B|A) = \\large\\frac{P(A|B) P(B)}{P(A)}\n",
    "$\n",
    "\n",
    "The marginal likelihood $P(A)$ might be difficult to compute. We can introduce unnormalized conditional probabilities as follow:\n",
    "\n",
    "$\n",
    "P'(B|A) = P(A|B) \\cdot P(B) \\\\\n",
    "P'(\\lnot B|A) = P(A|\\lnot B) \\cdot P(\\lnot B)\n",
    "$\n",
    "\n",
    "Because these are two complementary events:\n",
    "\n",
    "$\n",
    "P(B|A) + P(\\lnot B|A) = 1\n",
    "$\n",
    "\n",
    "And so:\n",
    "\n",
    "$\n",
    "P(B|A) = P'(B|A) \\cdot \\large \\frac{1}{P'(B|A) + P'(\\lnot B|A)} = \\large \\frac{P(A|B) \\cdot P(B)}{P'(B|A) + P'(\\lnot B|A)}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "Math Jax [doc](https://math.meta.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-referencehttps://math.meta.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
