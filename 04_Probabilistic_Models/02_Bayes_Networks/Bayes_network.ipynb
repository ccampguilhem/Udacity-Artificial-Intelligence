{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cascading rule\n",
    "\n",
    "Let X and Y be 2 dependent variables. The joint probability is given by:\n",
    "\n",
    "$\n",
    "P(X=X_i, Y=Y_j) = P(X=X_i|Y=Y_j) \\cdot P(Y=Y_j)\n",
    "$\n",
    "\n",
    "Let Z be a third dependent variable. The joint probability is given by:\n",
    "\n",
    "$\n",
    "P(X=X_i, Y=Y_j, Z=Z_k) = P(X=X_i|Y=Y_j, Z=Z_k) \\cdot P(Y=Y_j, Z=Z_k)\n",
    "$\n",
    "\n",
    "We can further develop to:\n",
    "\n",
    "$\n",
    "P(X=X_i, Y=Y_j, Z=Z_k) = P(X=X_i|Y=Y_j, Z=Z_k) \\cdot P(Y=Y_j|Z=Z_k) \\cdot P(Z=Z_k)\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dentist example\n",
    "\n",
    "Consider the following joint probability:\n",
    "    \n",
    "$\n",
    "P(Detect, Pain, Decay, Competent)\n",
    "$\n",
    "\n",
    "$P(Detect)$ is the probability that dentist detects an anomaly, $P(Pain)$ is the probability that patient suffers pain, $P(Decay)$ is the probability that patient has a tooth decay and $P(Competent)$ is the probability of the dentist to be competent.\n",
    "\n",
    "Applying the cascading rule, we can write:\n",
    "\n",
    "$\n",
    "P(Detect, Pain, Decay, Competent) \\\\\n",
    "= P(Detect|Pain, Decay, Competent) \\cdot P(Pain, Decay, Competent) \\\\\n",
    "= P(Detect|Pain, Decay, Competent) \\cdot P(Pain|Decay, Competent) \\cdot P(Decay, Competent) \\\\\n",
    "= P(Detect|Pain, Decay, Competent) \\cdot P(Pain|Decay, Competent) \\cdot P(Decay|Competent) \\cdot P(Comptent)\n",
    "$\n",
    "\n",
    "We can note the following:\n",
    "\n",
    "- $P(Decay|Competent)$: there should be no relation between the competence of the dentist and the fact patient has a tooth decay or not\n",
    "- $P(Pain|Decay, Comptent)$: there might be a relation between having a tooth decay and suffering pain, but this should be independent on the dentist competence\n",
    "- $P(Detect|Pain, Decay, Competent)$: there is a relationship between the fact dentist detects an anomaly based on the patient having a tooth decay and the dentist being competent. But the fact patient is suffering pain is independent on the fact dentist detects an anomaly.\n",
    "\n",
    "We can then simplify the joint probaility to:\n",
    "\n",
    "$\n",
    "P(Detect, Pain, Decay, Competent) \\\\\n",
    "= P(Detect|Pain, Decay, Competent) \\cdot P(Pain|Decay, Competent) \\cdot P(Decay|Competent) \\cdot P(Comptent) \\\\\n",
    "= P(Detect|Decay, Competent) \\cdot P(Pain|Decay) \\cdot P(Decay) \\cdot P(Comptent)\n",
    "$\n",
    "\n",
    "We can conclude that:\n",
    "\n",
    "- the fact dentist detects an anomaly depends on the patient having a tooth decay and the dentist being competent\n",
    "- the fact patient suffers pain depends on the patient having a tooth decay\n",
    "- the patient having a tooth decay is independent to the other variables\n",
    "- the dentist being competent is independent to the other variables\n",
    "\n",
    "We can report this into a graph:\n",
    "\n",
    "<img src=\"./images/dentist_network.svg\" style=\"height: 200px\">\n",
    "\n",
    "For this to be an actual Bayesian Network, we need to add the probabilities:\n",
    "\n",
    "<img src=\"./images/dentist_network2.svg\" style=\"height: 300px\">\n",
    "\n",
    "A node in this graph with no predecessor requires 1 parameter (one probability), a node with 1 predecessor requires 2 and a node with 2 predecessors requires 4 parameters. In this example the Bayesian network requires 8 parameters.\n",
    "\n",
    "The number of parameters required is $2^{number of predecessors}$.\n",
    "\n",
    "As we have 4 variables, each having two possible values, the total number of probabilities that we would have need is $2^4 = 16$. As the sum of probabilities must be 1, the total number of probabilities thta we would have needed is then $2^n-1$. A Bayesian Network can then be seen as a factorized view as it requires only 8 probabilities instead of 15. From the 8 probability we are able to infer all the 16 probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional dependence\n",
    "\n",
    "Let's consider the following Bayes Network:\n",
    "\n",
    "<img src=\"./images/conditional_dependence01.svg\" style=\"height: 200px\">\n",
    "\n",
    "The network implies that if we know A, B and C are independent. This can be written like this:\n",
    "\n",
    "$$\n",
    "B \\bot C | A\n",
    "$$\n",
    "\n",
    "B and C are conditionaly independent. But this does not imply that B and C are absolute independent. Actually, if we know B, it means that we might have a more accurate idea of what A might be. A because of that we also have a better understanding of what C could be.\n",
    "\n",
    "We can actually compute the probability of C given B:\n",
    "\n",
    "$\n",
    "P(C|B) = P(C|B,A) \\cdot P(A|B) + P(C|B,\\lnot A) \\cdot P(\\lnot A|B)\n",
    "$\n",
    "\n",
    "This formulation actually reflects the following. To know C from B, we need to have a look at A. A can be either true or false. The probability of C knowing B is then the sum of the probability of C knowing A and B times the probability that A is true given B and the probability of C knowing B and not A times the probability of A being false knowing B. Everything is written as if we know B."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confounding cause\n",
    "\n",
    "Let's now consider this network:\n",
    "\n",
    "<img src=\"./images/conditional_dependence02.svg\" style=\"height: 200px\">\n",
    "\n",
    "B and C are independent. But if we know C, then A and B become linked. If there are two potential causes A and B leading to C event and we know that C has occured:\n",
    "\n",
    "- if A occured as well, then B is less likely to have occured\n",
    "- if B occured as well, then A is less likely to have occured\n",
    "\n",
    "This is known as the explain away effect and so:\n",
    "\n",
    "- $A \\bot B$: **True**: A and B are absolute independent\n",
    "- $A \\bot B | C$: **False**: A and B are dependent when C is given, this is the explain away effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d-Separation\n",
    "\n",
    "The d-separation technique consists in finding absolute or conditional independences in a Bayes network. D stands for dependence. Two nodes in a Bayes Network are dependent if they are connected by an *active* path of edges. *active* edges means that all nodes on the path are *active*. A node is by default *active* when there is no known variable (no givens) unless it's a collider in which case it is by default *inactive*. A collider is a node with confounding causes (more than one upstream connection). If a node becomes part of a set of givens, its status toggles. A node with only one upstream connection becomes *inactive* and a collider node becomes *active*.\n",
    "\n",
    "Let's consider the following examples from the Udacity course:\n",
    "\n",
    "<img src=\"./images/d-separation01.svg\" style=\"height: 200px\">\n",
    "\n",
    "We can say the following about this graph is there is no given:\n",
    "\n",
    "- $C \\bot E$: **False** there is a path with only active nodes connected C and E nodes, C and E are d-connected.\n",
    "- $B \\bot D$: **False** there is a path with only active nodes connected B and D nodes\n",
    "- $A \\bot C$: **False** there is a path with only active nodes connected A and C nodes\n",
    "- $A \\bot E$: **False** there is a path with only active nodes connected A and E nodes\n",
    "\n",
    "Now let's assume than the given set includes B:\n",
    "\n",
    "- $A \\bot C | B$: **True** the only path connecting A and C passes through a inactive node B. A and C are d-separated given B.\n",
    "- $C \\bot E | B$: **True** the only path connecting C andE passes through a inactive node B. C and E are d-separated given B.\n",
    "\n",
    "C is only influenced by B. So if B is known, the knowledge of A brings no additional knowledge to C, that's the reason A and C are independent given B. And there is no way than knowing E would bring additional knowledge on C either, that's why they also are d-separated given B.\n",
    "\n",
    "A second example:\n",
    "\n",
    "<img src=\"./images/d-separation02.svg\" style=\"height: 200px\">\n",
    "\n",
    "We can say the following about this graph is there is no given:\n",
    "\n",
    "- $A \\bot B$: **True**: A and B are only connected through C which is a collider and then it is inactive in the set of givens is empty. A and B are d-separated.\n",
    "- $A \\bot E$: **False**: A influences C which in turn influences E. In the path connecting A et E (A -> C -> E), C is not a collider as it only has 1 upstream connection. A and E are d-connected, they are dependent.\n",
    "- $D \\bot E$: **False**: D and E are connected only through active nodes. In the path D -> C -> E, C is not considered as a collider (no upstram connection). D and E are dependent (d-connected )by the virtue of C.\n",
    "\n",
    "Let's now consider that B is given:\n",
    "\n",
    "- $A \\bot E | B$: **False**: A influences C which in turn influences E. In the path connecting A et E (A -> C -> E), C is not a collider as it only has 1 upstream connection. A and E are d-connected, they are dependent. The fact B is known does not change anything. The connection between A and E doest not pass through B.\n",
    "\n",
    "Let's now consider that C is given:\n",
    "\n",
    "- $A \\bot E | C$: **True**: The only connection between A and C is through C. Now C is *inactive* because it is not considered as a collider node (only one upstram connection) and it is part of the givens set. Knowing A does not bring any additional information to E, because E already knows that C is true.\n",
    "- $A \\bot B | C$: **False**: The path through A and B passes by C. In that case C is a collider (two upstream connections). And because C is also part of the set of givens, it means C is an active node. And so A and B are d-connected. This is the **explain away** effect. Two events A and B which are absolute independent become dependent when an event C is known and A and B are confounding causes for C. C being given, knowing A increases or reduces the odds that B also occured.\n",
    "\n",
    "Another way to see d-dependence is considering active or inactive triplets. On the left hand side of the following picture, the events are dependent. On the roght hand side they are independent. Plain nodes are known and empty ones are unkown.\n",
    "\n",
    "<img src=\"./images/d-separation.png\" style=\"height: 400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferences with Bayesian networks\n",
    "\n",
    "In this section, we will consider the following example:\n",
    "\n",
    "<img src=\"./images/bayes_net01.png\">\n",
    "\n",
    "### Enumeration technique\n",
    "\n",
    "$\n",
    "P(A,B,C|D,E) = \\large \\frac{P(A,B,C,D,E)}{P(D,E)}\n",
    "$\n",
    "\n",
    "In a Bayes network, A, B and C are the query variables. D and E are the evidence. The conditional probabilities of the queries given the evidences is the ratio between the joint probability of all variables over the joint probabilities of the evidences.\n",
    "\n",
    "Let's take an example and try to calculate the probability $P(+b|+j,+m)$ which is the probability that a burglary occured given John and Mary called:\n",
    "\n",
    "$\n",
    "P(+b|+j,+m) = \\large \\frac{P(+b,+j,+m)}{P(+j,+m)}\n",
    "$\n",
    "\n",
    "To calculate the joint probabuility $P(+b,+j,+m)$ using the enumeration technique, we need to sum the probabilties over the hidden variables to the problem:\n",
    "\n",
    "$\n",
    "P(+b,+j,+m) = \\sum_{a} \\sum_{e} P(+b,+j,+m, a, e)\n",
    "$\n",
    "\n",
    "Using the cascading rule for the probabilities, this leads to:\n",
    "\n",
    "$\n",
    "P(+b,+j,+m) = \\sum_{a} \\sum_{e} P(+b) \\cdot P(e) \\cdot P(a|+b,e) \\cdot P(+j|a) \\cdot P(+m|a)\n",
    "$\n",
    "\n",
    "It turns out it is the product of conditional probabilities of each node in the Bayes Network given upstream nodes in the network. This product of probabilities is the factor f, so the equation becomes:\n",
    "\n",
    "$\n",
    "P(+b,+j,+m) = \\sum_{a} \\sum_{e} f(e, a) \\\\\n",
    "= f(+e,+a) + f(+e,\\lnot a) + f(\\lnot e,+a) + f(\\lnot e,\\lnot a)\n",
    "$\n",
    "\n",
    "For example:\n",
    "\n",
    "$\n",
    "f(+e,+a) = P(+b) \\cdot P(+e) \\cdot P(+a|+b,+e) \\cdot P(+j|+a) \\cdot P(+m|+a) \\\\\n",
    "= 0.001 \\cdot 0.002 \\cdot 0.95 \\cdot 0.9 \\cdot 0.7 \\\\\n",
    "= 0.000001197\n",
    "$\n",
    "\n",
    "This is only one of the four terms that we would need to calculate $P(+b,+j,+m)$, so there is still along way to go before we can reach $P(+b|+j,+m)$\n",
    "\n",
    "### Variable elimination\n",
    "\n",
    "Doing enumeration in a large net might be tedious, another technique exists to speed up the process, this is variable elimination.\n",
    "\n",
    "Let's take the following example:\n",
    "\n",
    "<img src=\"./images/conditional_dependence03.svg\" style=\"height: 300px\">\n",
    "\n",
    "To solve the questions we are going to use the *elimination of variables* technique. We introduce the following factors:\n",
    "\n",
    "f1(sunny):\n",
    "\n",
    "sunny | p\n",
    "------|----\n",
    "T     | 0.7\n",
    "F     | 0.3\n",
    "\n",
    "f2(raise):\n",
    "\n",
    "raise | p\n",
    "------|----\n",
    "T     | 0.01\n",
    "F     | 0.99\n",
    "\n",
    "f3(happy, sunny, raise):\n",
    "\n",
    "happy | sunny | raise | p\n",
    "------|-------|-------|-----\n",
    "T     | T     | T     | 1.0\n",
    "T     | T     | F     | 0.7\n",
    "T     | F     | T     | 0.9\n",
    "T     | F     | F     | 0.1\n",
    "F     | T     | T     | 0.0\n",
    "F     | T     | F     | 0.3\n",
    "F     | F     | T     | 0.1\n",
    "F     | F     | F     | 0.9\n",
    "\n",
    "The factors are table of probabilities.\n",
    "\n",
    "The probability $P(R|S)$ that I got a raise knowing that is sunny is 0.01, which is the probability of getting a raise. Both events *raise* and *sunny* are independent.\n",
    "\n",
    "What is the probability $P(R|H, S)$ of getting a raise knowing that I am happy and it is sunny ?\n",
    "\n",
    "There is a single factor f3 which is a function of H. But as we know H, we can eliminate a few lines from the table:\n",
    "\n",
    "f3'(sunny, raise):\n",
    "\n",
    "sunny | raise | p\n",
    "------|-------|-----\n",
    "T     | T     | 1.0\n",
    "T     | F     | 0.7\n",
    "F     | T     | 0.9\n",
    "F     | F     | 0.1\n",
    "\n",
    "We also know that it is sunny, but we have two factors f3' and f1 depending on S so we multiply the factors (which is the inner product of the two tables) and the probability is the product of all probabilities. This new factor is f4:\n",
    "\n",
    "f4(sunny, raise):\n",
    "\n",
    "sunny | raise | f3' | f1   | p\n",
    "------|-------|-----|------|------\n",
    "T     | T     | 1.0 | 0.7  | 0.7\n",
    "T     | F     | 0.7 | 0.7  | 0.49\n",
    "F     | T     | 0.9 | 0.3  | 0.27\n",
    "F     | F     | 0.1 | 0.3  | 0.03\n",
    "\n",
    "And because we know S, we can eliminate the variable from the factor so that we have f4':\n",
    "\n",
    "f4'(raise):\n",
    "\n",
    "raise | p\n",
    "------|-----\n",
    "T     | 0.7\n",
    "F     | 0.49\n",
    "\n",
    "If we want to know the probability $P(R|H,S)$ we also need to take into account all the factors depending on *raise* doing a inner product. This result into the f5 factor:\n",
    "\n",
    "f5(raise):\n",
    "\n",
    "raise | f4'  | f2   | p\n",
    "------|------|------|-----\n",
    "T     | 0.7  | 0.01 | 0.007\n",
    "F     | 0.49 | 0.99 | 0.4581\n",
    "\n",
    "We need to normalize the probability so:\n",
    "\n",
    "$\n",
    "P(R|H,S) = \\large \\frac{0.007}{0.007 + 0.4581} = 0.0142\n",
    "$\n",
    "\n",
    "Let's know introduce f6 that is product of all factors:\n",
    "\n",
    "f6(happy, raise, sunny):\n",
    "\n",
    "happy | sunny | raise | p\n",
    "------|-------|-------|-----\n",
    "T     | T     | T     | 1.0 * 0.7 * 0.01 = 0.007\n",
    "T     | T     | F     | 0.7 * 0.7 * 0.99 = 0.4851\n",
    "T     | F     | T     | 0.9 * 0.3 * 0.01 = 0.0027\n",
    "T     | F     | F     | 0.1 * 0.3 * 0.99 = 0.0297\n",
    "F     | T     | T     | 0.0 * 0.7 * 0.01 = 0.0\n",
    "F     | T     | F     | 0.3 * 0.7 * 0.99 = 0.2079\n",
    "F     | F     | T     | 0.1 * 0.3 * 0.01 = 0.0003\n",
    "F     | F     | F     | 0.9 * 0.3 * 0.99 = 0.2673\n",
    "\n",
    "What is the probability of getting a raise if I am happy $P(R|H)$ ? From the factor f6 table, we can eliminate the variables where H is False:\n",
    "\n",
    "f6'(raise, sunny):\n",
    "\n",
    "sunny | raise | p\n",
    "------|-------|-----\n",
    "T     | T     | 0.007\n",
    "T     | F     | 0.4851\n",
    "F     | T     | 0.0027\n",
    "F     | F     | 0.0297\n",
    "\n",
    "As we know nothing about weather, we will add all the probabilities for a given value of raise:\n",
    "\n",
    "f6''(raise):\n",
    "\n",
    "raise | p\n",
    "------|-----\n",
    "T     | 0.007 + 0.0027 = 0.0097\n",
    "F     | 0.4851 + 0.0297 = 0.5148\n",
    "\n",
    "And so:\n",
    "\n",
    "$\n",
    "P(R|H) = \\large \\frac{0.0097}{0.0097 + 0.5148} = 0.0185\n",
    "$\n",
    "\n",
    "If we don't know about the wheather, the probability of getting a raise if I'm happy is 0.0185. The probability of getting a raise drops to 0.0142 if we also know that it's sunny.\n",
    "\n",
    "When making an inference like this in a Bayesian network:\n",
    "\n",
    "- H is called the **evidence**: this is what we know\n",
    "- R is the **query**: this is what we are looking for\n",
    "- S is a **hidden variable**: we know nothing about it and we are basically not interested by it\n",
    "\n",
    "### Approximate inference\n",
    "\n",
    "Even for very large networks, variable elimination might take a long time to process. Another technique consists in making an approximate inference. The approximation is done thanks to a sampling.\n",
    "\n",
    "Let's consider the following example:\n",
    "\n",
    "<img src=\"./images/bayes_net02.svg\" style=\"height: 300px\">\n",
    "\n",
    "A first example would be to calculate the probability that the grass is wet $P(W)$:\n",
    "\n",
    "- we start off with the variable for which all parents are defined, in that case variable C\n",
    "- we randomly select the C variable so that it has 50% chances of being cloudy: let's say it's cloudy\n",
    "- we randomly select the S variable limiting the cases where it's cloudy. There is 10% of chances that sprinlers are turned on. So let's assume that they are turned off.\n",
    "- we randomly select the R variable following the same process than with S. There is 80% of chances that it's rainy if it's cloudy, so let's assume that is is actually raining.\n",
    "- finally, we randomly select the W variable taking into account that sprinklers are turned off and that it's rainy. That leaves us with a probability of 90% that the grass is wet, so let's assume it is actually wet.\n",
    "- we repeat the same process over and over.\n",
    "- we can then calculate the probability that the grass is wet by dividing the number of samples for which grass was wet by the total number of samples.\n",
    "- this process ensure consistency with an actual inference in the Bayesian network.\n",
    "\n",
    "In a second example, we want to calculate the conditional probability $P(W|\\lnot C)$. We can actually take the sampling that we have done earlier and reject all samples for which weather was cloudy. We can then calculate the probability of the grass getting wet in the same way we did before. This procedure is still consistent.\n",
    "\n",
    "But let's assume that the probability that weather is cloudy is really small. We would have to reject a lot of samples, and maybe we would be left with a very small number of them which can not lead to an accurate value for the probability.\n",
    "\n",
    "We can then proceed with a likelihood weigthing. Instead of randomly selecting all the variables, we are going to fix the given values. The problem with that is that it results in a sampling that is inconsistent. To make it consistent, we need to add a weight for each sample.\n",
    "\n",
    "In a third example, we are going to calculate the conditional probability $P(R|S,W)$:\n",
    "\n",
    "- we initialize the weight of the sample to 1.0\n",
    "- we start off with the variable for which all parents are defined, in that case variable C\n",
    "- we randomly select the C variable so that it has 50% chances of being cloudy: let's say it's cloudy\n",
    "- S variable is fixed: it is a given and the upstream node C has been set. We multiply the weight of the sample by the probability P(S|C) because this is the row matching the constraints that we have.\n",
    "- R is randomly generated taking into account that it is cloudy. Let's assume it's raining.\n",
    "- W variable is fixed: it is a given and both parents have been fixed S and R. We multiply the weight sample by P(W|R,S) because it is the row that we are constrained to choose.\n",
    "- we repeat the process over and over\n",
    "- we calculate the conditional probability taking into account the weight of each sample.\n",
    "- this process ensure consistency with an actual inference in the Bayesian network.\n",
    "\n",
    "In a fourth example, we want to calculate the conditional probability $P(C|R,S)$:\n",
    "\n",
    "- we could randomly select the variable W given the constraints and updating the sample weight accordingly\n",
    "- but, the generation of the C variable would be completly random and would not match the evidences that we have. This is a limitation of this random sapling technique.\n",
    "\n",
    "An alternative sampling method exists, its the Gibbs sampling and uses Markov Chain Monte Carlo algorithm: [Wikipedia](https://en.wikipedia.org/wiki/Gibbs_sampling) has a dedicated article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[MathJax tutorial](https://math.meta.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference)\n",
    "\n",
    "[Introduction to Bayesian Networks](https://www.youtube.com/watch?v=OjlC-4iIndU)\n",
    "\n",
    "[Bayesian inference](https://www.youtube.com/watch?v=dvi2k7OzBHA)\n",
    "\n",
    "[d-separation](http://web.mit.edu/jmn/www/6.034/d-separation.pdf) examples and [d-Separation](http://bayes.cs.ucla.edu/BOOK-2K/d-sep.html) without tears. But I found this [one](http://www.andrew.cmu.edu/user/scheines/tutor/d-sep.html#explanation) clearer to me and this is what I have reported in the d-Separation section.\n",
    "\n",
    "[Gibbs sampling](https://en.wikipedia.org/wiki/Gibbs_sampling) on Wikipedia."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
